# src/data_ingestion/newsapi_client.py
import logging
from newsapi import NewsApiClient as ApiClient
from typing import List, Dict, Any
from config import NEWSAPI_API_KEY, ASSETS_TO_TRACK # <-- –ù–û–í –ò–ú–ü–û–†–¢

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

ECONOMIC_NEWS_SOURCES = 'bloomberg,reuters,the-wall-street-journal,financial-times'

class NewsApiClient:
    def __init__(self):
        if not NEWSAPI_API_KEY or "–¢–í–û–Ø–¢" in NEWSAPI_API_KEY:
            raise ValueError("NewsAPI –∫–ª—é—á—ä—Ç –Ω–µ –µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–∞–Ω –≤ .env —Ñ–∞–π–ª–∞.")
        self.api = ApiClient(api_key=NEWSAPI_API_KEY)
        logging.info("üì∞ NewsAPI Client initialized.")

    # --- –ù–û–í –ú–ï–¢–û–î: –¶–µ–ª–µ–Ω–∞—Å–æ—á–µ–Ω–æ —Ç—ä—Ä—Å–µ–Ω–µ –∑–∞ –≤—Å–µ–∫–∏ –∞–∫—Ç–∏–≤ ---
    def fetch_asset_news(self) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–∏—á–∞ –Ω–æ–≤–∏–Ω–∏ –∑–∞ –í–°–ï–ö–ò –∞–∫—Ç–∏–≤, –¥–µ—Ñ–∏–Ω–∏—Ä–∞–Ω –≤ ASSETS_TO_TRACK.
        –ü—Ä–∞–≤–∏ –æ—Ç–¥–µ–ª–Ω–∞ –∑–∞—è–≤–∫–∞ –∑–∞ –≤—Å–µ–∫–∏, –∑–∞ –¥–∞ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç.
        """
        logging.info("üîç Fetching asset-specific news from NewsAPI...")
        all_articles = []
        # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ set, –∑–∞ –¥–∞ –∏–∑–±–µ–≥–Ω–µ–º –¥—É–±–ª–∏–∫–∞—Ç–∏, –∞–∫–æ –µ–¥–Ω–∞ —Å—Ç–∞—Ç–∏—è —Å–ø–æ–º–µ–Ω–∞–≤–∞ –¥–≤–∞ –∞–∫—Ç–∏–≤–∞
        processed_urls = set()

        # –í–∑–∏–º–∞–º–µ –∏–º–µ–Ω–∞—Ç–∞ –Ω–∞ –∞–∫—Ç–∏–≤–∏—Ç–µ –æ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ç–∞ (–Ω–∞–ø—Ä. 'bitcoin', 'ethereum')
        asset_keywords = list(ASSETS_TO_TRACK.keys())

        for asset in asset_keywords:
            logging.info(f"    -> Searching for '{asset}'...")
            try:
                response = self.api.get_everything(
                    q=f'"{asset}"',  # –¢—ä—Ä—Å–∏–º –∑–∞ —Ç–æ—á–Ω–æ—Ç–æ –∏–º–µ –Ω–∞ –∞–∫—Ç–∏–≤–∞
                    language='en',
                    sort_by='publishedAt',
                    page_size=15  # 15 –Ω–æ–≤–∏–Ω–∏ –Ω–∞ –∞–∫—Ç–∏–≤ —Å–∞ –¥–æ—Å—Ç–∞—Ç—ä—á–Ω–∏
                )
                
                # –ü–æ–¥–∞–≤–∞–º–µ –∏–º–µ—Ç–æ –Ω–∞ –∞–∫—Ç–∏–≤–∞, –∑–∞ –¥–∞ "–º–∞—Ä–∫–∏—Ä–∞–º–µ" –Ω–æ–≤–∏–Ω–∏—Ç–µ
                articles = self._process_articles(
                    response.get('articles', []),
                    category=asset, # <-- –¢–£–ö –ï –ö–õ–Æ–ß–û–í–ê–¢–ê –ü–†–û–ú–Ø–ù–ê
                    processed_urls=processed_urls
                )
                all_articles.extend(articles)
                logging.info(f"        Found {len(articles)} new articles for {asset}.")

            except Exception as e:
                logging.error(f"    -> ‚ùå Error fetching news for {asset}: {e}")
        
        return all_articles

    # --- –¢–æ–∑–∏ –º–µ—Ç–æ–¥ –æ—Å—Ç–∞–≤–∞ —Å—ä—â–∏—è—Ç ---
    def fetch_economic_news(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–∏—á–∞ —Å–∏–ª–Ω–æ —Ñ–æ–∫—É—Å–∏—Ä–∞–Ω–∏ –∏–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ –Ω–æ–≤–∏–Ω–∏ –æ—Ç —Ç–æ–ø –∏–∑—Ç–æ—á–Ω–∏—Ü–∏."""
        logging.info(f"üìà Searching NewsAPI for economic keywords: {keywords}")
        query_string = ' OR '.join(f'"{k}"' for k in keywords)
        try:
            response = self.api.get_everything(
                q=query_string, sources=ECONOMIC_NEWS_SOURCES,
                language='en', sort_by='relevancy', page_size=20
            )
            articles = self._process_articles(response.get('articles', []), category='economic_event')
            logging.info(f"   -> Found {len(articles)} economic news articles.")
            return articles
        except Exception as e:
            logging.error(f"   -> ‚ùå Error fetching economic news: {e}")
            return []

    # --- –õ–µ–∫–æ –ø–æ–¥–æ–±—Ä–µ–Ω–∏–µ –≤ _process_articles ---
    def _process_articles(self, articles_data: List[Dict[str, Any]], category: str, processed_urls: set = None) -> List[Dict[str, Any]]:
        """–ü–æ–º–æ—â–µ–Ω –º–µ—Ç–æ–¥ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –æ—Ç–≥–æ–≤–æ—Ä–∞ –æ—Ç API-—Ç–æ."""
        processed = []
        for article in articles_data:
            url = article.get('url')
            # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –∑–∞ –¥—É–±–ª–∏–∫–∞—Ç–∏, –∞–∫–æ –Ω–∏ –µ –ø–æ–¥–∞–¥–µ–Ω set
            if url and (processed_urls is None or url not in processed_urls):
                processed_article = {
                    'source': article.get('source', {}).get('name', 'N/A'),
                    'title': article.get('title'), 'url': url,
                    'published_at': article.get('publishedAt', 'N/A'),
                    'category': category  # –ó–∞–ø–∏—Å–≤–∞–º–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ç–∞ (–Ω–∞–ø—Ä. 'bitcoin')
                }
                
                if processed_article['title']:
                    processed.append(processed_article)
                    if processed_urls is not None:
                        processed_urls.add(url)
        return processed

# --- –û–±–Ω–æ–≤—è–≤–∞–º–µ —Ç–µ—Å—Ç–æ–≤–∏—è –±–ª–æ–∫ ---
if __name__ == '__main__':
    logging.info("--- Testing NewsAPI Client ---")
    try:
        client = NewsApiClient()
        asset_articles = client.fetch_asset_news()
        
        if asset_articles:
            logging.info(f"\nSuccessfully fetched {len(asset_articles)} asset-specific articles.")
            logging.info(f"Example article: {asset_articles[0]['title']} (Category: {asset_articles[0]['category']})")

    except ValueError as e:
        logging.error(e)